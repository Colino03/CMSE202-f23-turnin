{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment 4\n",
    "\n",
    "## Using the Perceptron, SVMs, and PCA with sonar data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"text-align: right;\"> &#9989; Colin Bonnema </p>\n",
    "### <p style=\"text-align: right;\"> &#9989; Colino03</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://images.squarespace-cdn.com/content/v1/5497331ae4b0148a6141bd47/1533488464157-AYYS0QO7ZW0KPGOT6XRQ/underwater-sea-mine-danger-weapon-deadly-naval-ocean-sea_bpocqo-8__F0000.png?format=1500w\" width=400px align=\"right\" style=\"margin-left: 20px\" alt=\"Underwater naval mines\">\n",
    "\n",
    "### Goals for this homework assignment\n",
    "\n",
    "By the end of this assignment, you should be able to:\n",
    "* Use `git` and the branching functionality to track your work and turn in your assignment\n",
    "* Read in data and prepare it for modeling\n",
    "* Build, fit, and evaluate an SVC model of data\n",
    "* Use PCA to reduce the number of important features\n",
    "* Build, fit, and evaluate an SVC model of PCA-transformed data\n",
    "* Systematically investigate the effects of the number of PCA components on an SVC model of data\n",
    "\n",
    "### Assignment instructions:\n",
    "\n",
    "Work through the following assignment, making sure to follow all of the directions and answer all of the questions.\n",
    "\n",
    "There are **59 points** possible on this assignment. Point values for each part are included in the section headers.\n",
    "\n",
    "This assignment is **due by 11:59 pm on Friday, December 1. It should be pushed to your repo (see Part 1) AND submitted to D2L**. \n",
    "\n",
    "#### Imports\n",
    "\n",
    "It's useful to put all of the imports you need for this assignment in one place. Read through the assignment to figure out which imports you'll need or add them here as you go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all necessary imports here\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Git Repo Management and Branching (6 points)\n",
    "\n",
    "For this assignment, you're going to add it to the `cmse202-f23-turnin` repository you created in class so that you can track your progress on the assignment and preserve the final version that you turn in. In order to do this you need to\n",
    "\n",
    "**&#9989; Do the following**:\n",
    "\n",
    "1. Navigate to your `cmse202-f23-turnin` **local** repository and create a new directory called `hw-04`\n",
    "\n",
    "2. Move this notebook into that **new directory** in your repository, but **do not** add or commit it to your repository yet.\n",
    "\n",
    "3. Create a **new branch** called `hw04_branch` (The Day 16 PCA and ICA content has information on how to do this).\n",
    "\n",
    "4. \"Check out\" the new branch (so that you'll be working on that branch). \n",
    "\n",
    "5. Double check to make sure you are actually on that branch.\n",
    "\n",
    "6. Once you're certain you're working on your new branch, add this notebook to your repository, then make a commit and push it to GitHub. You may need to use `git push origin hw04_branch` to push your new branch to GitHub.\n",
    "\n",
    "Finally, &#9989; **Do this**: Before you move on, put the command that your instructor should run to clone your repository in the markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# https://github.com/Colino03/CMSE202-f23-turnin.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: Double check you've added your Professor and your TA as collaborators to your \"turnin\" repository (you should have done this in the previous homework assignment).\n",
    "\n",
    "**Also important**: Make sure that the version of this notebook that you are working on is the same one that you just added to your repository! If you are working on a different copy of the notebook, **none of your changes will be tracked**!\n",
    "\n",
    "If everything went as intended, the file should now show up on your GitHub account in the \"`cmse202-f23-turnin`\" repository inside the `hw-04` directory that you just created within the new branch `hw04_branch`.\n",
    "\n",
    "Periodically, **you'll be asked to commit your changes to the repository and push them to the remote GitHub location**. Of course, you can always commit your changes more often than that, if you wish.  It can be good to get into a habit of committing your changes any time you make a significant modification, or when you stop working on the project for a bit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"loading\"></a>\n",
    "## 2. Loading a the dataset: Sonar data measurements (7 points)\n",
    "\n",
    "You may or may not have had a chance to experiment with the sonar dataset provided in the Day 19 ICA extension assignment, but it's another common dataset used for testing out **binary classifiers**.\n",
    "\n",
    "Since the goal for this assignment is to practice using the Perceptron classifier, SVMs, and PCA tools we've covered in class, we'll going to use this relatively simple dataset and avoid any complicated data wrangling headaches!\n",
    " \n",
    "#### The data\n",
    "\n",
    "The sonar dataset is pretty straight forward, but you'll need to download the data and give yourself some time to get familiar with it.\n",
    "\n",
    "**&#9989; Do This:**  To get started, **you'll need to download the following file**:\n",
    "\n",
    "`https://raw.githubusercontent.com/msu-cmse-courses/cmse202-supplemental-data/main/data/sonar.csv`\n",
    "\n",
    "Once you've downloaded the data, **open the files using a text browser or other tool on your computer and take a look at the data to get a sense for the information it contains.** Make sure you take a moment to read the [UC Irvine Machine Learning Repository page](http://archive.ics.uci.edu/dataset/151/connectionist+bench+sonar+mines+vs+rocks) to understand exactly what is in this dataset, but essentially is a collection of sonar measurements of rocks and \"mines\" (metal cynlinders). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 2.0 (2 points):** In your own words, what is the information contains in the sonar dataset? What does each column represent? What are the current labels in the dataset? Are they numeric or string format? What are the possible values for the labels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> Each row has 60 unique energy readings corresponding to a specific frequency band, ranging from 0 to 1. They are in numeric format, witht the titles/attributes being in string format. The Class column has strings that are either Rock or Mine. These can be changed to 1 and 0 to make them numeric and easier to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load the data\n",
    "\n",
    "**&#9989; Task 2.1 (1 point):** Read the ```sonar.csv``` file into your notebook. Perhaps unsurprisingly, we're going to use \"Class\" column as the classes that we'll be trying to predict with our classification model.\n",
    "\n",
    "Once you've loaded in the data, **display the DataFrame to make sure it looks reasonable**. You should have **61 columns** and **208 rows**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribute_1</th>\n",
       "      <th>attribute_2</th>\n",
       "      <th>attribute_3</th>\n",
       "      <th>attribute_4</th>\n",
       "      <th>attribute_5</th>\n",
       "      <th>attribute_6</th>\n",
       "      <th>attribute_7</th>\n",
       "      <th>attribute_8</th>\n",
       "      <th>attribute_9</th>\n",
       "      <th>attribute_10</th>\n",
       "      <th>...</th>\n",
       "      <th>attribute_52</th>\n",
       "      <th>attribute_53</th>\n",
       "      <th>attribute_54</th>\n",
       "      <th>attribute_55</th>\n",
       "      <th>attribute_56</th>\n",
       "      <th>attribute_57</th>\n",
       "      <th>attribute_58</th>\n",
       "      <th>attribute_59</th>\n",
       "      <th>attribute_60</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>Mine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1018</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>Mine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>Mine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>Mine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.1843</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>Mine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     attribute_1  attribute_2  attribute_3  attribute_4  attribute_5  \\\n",
       "0         0.0200       0.0371       0.0428       0.0207       0.0954   \n",
       "1         0.0453       0.0523       0.0843       0.0689       0.1183   \n",
       "2         0.0262       0.0582       0.1099       0.1083       0.0974   \n",
       "3         0.0100       0.0171       0.0623       0.0205       0.0205   \n",
       "4         0.0762       0.0666       0.0481       0.0394       0.0590   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "203       0.0187       0.0346       0.0168       0.0177       0.0393   \n",
       "204       0.0323       0.0101       0.0298       0.0564       0.0760   \n",
       "205       0.0522       0.0437       0.0180       0.0292       0.0351   \n",
       "206       0.0303       0.0353       0.0490       0.0608       0.0167   \n",
       "207       0.0260       0.0363       0.0136       0.0272       0.0214   \n",
       "\n",
       "     attribute_6  attribute_7  attribute_8  attribute_9  attribute_10  ...  \\\n",
       "0         0.0986       0.1539       0.1601       0.3109        0.2111  ...   \n",
       "1         0.2583       0.2156       0.3481       0.3337        0.2872  ...   \n",
       "2         0.2280       0.2431       0.3771       0.5598        0.6194  ...   \n",
       "3         0.0368       0.1098       0.1276       0.0598        0.1264  ...   \n",
       "4         0.0649       0.1209       0.2467       0.3564        0.4459  ...   \n",
       "..           ...          ...          ...          ...           ...  ...   \n",
       "203       0.1630       0.2028       0.1694       0.2328        0.2684  ...   \n",
       "204       0.0958       0.0990       0.1018       0.1030        0.2154  ...   \n",
       "205       0.1171       0.1257       0.1178       0.1258        0.2529  ...   \n",
       "206       0.1354       0.1465       0.1123       0.1945        0.2354  ...   \n",
       "207       0.0338       0.0655       0.1400       0.1843        0.2354  ...   \n",
       "\n",
       "     attribute_52  attribute_53  attribute_54  attribute_55  attribute_56  \\\n",
       "0          0.0027        0.0065        0.0159        0.0072        0.0167   \n",
       "1          0.0084        0.0089        0.0048        0.0094        0.0191   \n",
       "2          0.0232        0.0166        0.0095        0.0180        0.0244   \n",
       "3          0.0121        0.0036        0.0150        0.0085        0.0073   \n",
       "4          0.0031        0.0054        0.0105        0.0110        0.0015   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "203        0.0116        0.0098        0.0199        0.0033        0.0101   \n",
       "204        0.0061        0.0093        0.0135        0.0063        0.0063   \n",
       "205        0.0160        0.0029        0.0051        0.0062        0.0089   \n",
       "206        0.0086        0.0046        0.0126        0.0036        0.0035   \n",
       "207        0.0146        0.0129        0.0047        0.0039        0.0061   \n",
       "\n",
       "     attribute_57  attribute_58  attribute_59  attribute_60  Class  \n",
       "0          0.0180        0.0084        0.0090        0.0032   Rock  \n",
       "1          0.0140        0.0049        0.0052        0.0044   Rock  \n",
       "2          0.0316        0.0164        0.0095        0.0078   Rock  \n",
       "3          0.0050        0.0044        0.0040        0.0117   Rock  \n",
       "4          0.0072        0.0048        0.0107        0.0094   Rock  \n",
       "..            ...           ...           ...           ...    ...  \n",
       "203        0.0065        0.0115        0.0193        0.0157   Mine  \n",
       "204        0.0034        0.0032        0.0062        0.0067   Mine  \n",
       "205        0.0140        0.0138        0.0077        0.0031   Mine  \n",
       "206        0.0034        0.0079        0.0036        0.0048   Mine  \n",
       "207        0.0040        0.0036        0.0061        0.0115   Mine  \n",
       "\n",
       "[208 rows x 61 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put your code here\n",
    "sonar = pd.read_csv('sonar.csv')\n",
    "sonar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Relabeling the classes\n",
    "\n",
    "To simplify the process of modeling the sonar data, we should convert the class labels from strings to integers. For example, rather than `Rock`, we can consider this to be class \"`1`\".\n",
    "\n",
    "**&#9989; Task 2.2 (2 points):** Replace all of the strings in your \"Class\" column with integers based on the following:\n",
    "\n",
    "| original label | replaced label |\n",
    "| -------- | -------- |\n",
    "| Rock | 1 |\n",
    "| Mine | 0 |\n",
    "\n",
    "Once you've replaced the labels, display your DataFrame and confirm that it looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribute_1</th>\n",
       "      <th>attribute_2</th>\n",
       "      <th>attribute_3</th>\n",
       "      <th>attribute_4</th>\n",
       "      <th>attribute_5</th>\n",
       "      <th>attribute_6</th>\n",
       "      <th>attribute_7</th>\n",
       "      <th>attribute_8</th>\n",
       "      <th>attribute_9</th>\n",
       "      <th>attribute_10</th>\n",
       "      <th>...</th>\n",
       "      <th>attribute_52</th>\n",
       "      <th>attribute_53</th>\n",
       "      <th>attribute_54</th>\n",
       "      <th>attribute_55</th>\n",
       "      <th>attribute_56</th>\n",
       "      <th>attribute_57</th>\n",
       "      <th>attribute_58</th>\n",
       "      <th>attribute_59</th>\n",
       "      <th>attribute_60</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1018</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.1843</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     attribute_1  attribute_2  attribute_3  attribute_4  attribute_5  \\\n",
       "0         0.0200       0.0371       0.0428       0.0207       0.0954   \n",
       "1         0.0453       0.0523       0.0843       0.0689       0.1183   \n",
       "2         0.0262       0.0582       0.1099       0.1083       0.0974   \n",
       "3         0.0100       0.0171       0.0623       0.0205       0.0205   \n",
       "4         0.0762       0.0666       0.0481       0.0394       0.0590   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "203       0.0187       0.0346       0.0168       0.0177       0.0393   \n",
       "204       0.0323       0.0101       0.0298       0.0564       0.0760   \n",
       "205       0.0522       0.0437       0.0180       0.0292       0.0351   \n",
       "206       0.0303       0.0353       0.0490       0.0608       0.0167   \n",
       "207       0.0260       0.0363       0.0136       0.0272       0.0214   \n",
       "\n",
       "     attribute_6  attribute_7  attribute_8  attribute_9  attribute_10  ...  \\\n",
       "0         0.0986       0.1539       0.1601       0.3109        0.2111  ...   \n",
       "1         0.2583       0.2156       0.3481       0.3337        0.2872  ...   \n",
       "2         0.2280       0.2431       0.3771       0.5598        0.6194  ...   \n",
       "3         0.0368       0.1098       0.1276       0.0598        0.1264  ...   \n",
       "4         0.0649       0.1209       0.2467       0.3564        0.4459  ...   \n",
       "..           ...          ...          ...          ...           ...  ...   \n",
       "203       0.1630       0.2028       0.1694       0.2328        0.2684  ...   \n",
       "204       0.0958       0.0990       0.1018       0.1030        0.2154  ...   \n",
       "205       0.1171       0.1257       0.1178       0.1258        0.2529  ...   \n",
       "206       0.1354       0.1465       0.1123       0.1945        0.2354  ...   \n",
       "207       0.0338       0.0655       0.1400       0.1843        0.2354  ...   \n",
       "\n",
       "     attribute_52  attribute_53  attribute_54  attribute_55  attribute_56  \\\n",
       "0          0.0027        0.0065        0.0159        0.0072        0.0167   \n",
       "1          0.0084        0.0089        0.0048        0.0094        0.0191   \n",
       "2          0.0232        0.0166        0.0095        0.0180        0.0244   \n",
       "3          0.0121        0.0036        0.0150        0.0085        0.0073   \n",
       "4          0.0031        0.0054        0.0105        0.0110        0.0015   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "203        0.0116        0.0098        0.0199        0.0033        0.0101   \n",
       "204        0.0061        0.0093        0.0135        0.0063        0.0063   \n",
       "205        0.0160        0.0029        0.0051        0.0062        0.0089   \n",
       "206        0.0086        0.0046        0.0126        0.0036        0.0035   \n",
       "207        0.0146        0.0129        0.0047        0.0039        0.0061   \n",
       "\n",
       "     attribute_57  attribute_58  attribute_59  attribute_60  Class  \n",
       "0          0.0180        0.0084        0.0090        0.0032      1  \n",
       "1          0.0140        0.0049        0.0052        0.0044      1  \n",
       "2          0.0316        0.0164        0.0095        0.0078      1  \n",
       "3          0.0050        0.0044        0.0040        0.0117      1  \n",
       "4          0.0072        0.0048        0.0107        0.0094      1  \n",
       "..            ...           ...           ...           ...    ...  \n",
       "203        0.0065        0.0115        0.0193        0.0157      0  \n",
       "204        0.0034        0.0032        0.0062        0.0067      0  \n",
       "205        0.0140        0.0138        0.0077        0.0031      0  \n",
       "206        0.0034        0.0079        0.0036        0.0048      0  \n",
       "207        0.0040        0.0036        0.0061        0.0115      0  \n",
       "\n",
       "[208 rows x 61 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put your code here\n",
    "sonar['Class'] = sonar['Class'].replace('Rock',1)\n",
    "sonar['Class'] = sonar[\"Class\"].replace('Mine',0)\n",
    "sonar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Separating the \"features\" from the \"labels\"\n",
    "\n",
    "As we've seen when working with `sklearn` it can be much easier to work with the data if we have separate variables that store the features and the labels.\n",
    "\n",
    "**&#9989; Task 2.3 (1 point):** Split your DataFrame so that you have two separate DataFrames, one called `features`, which contains all of the sonar features, and one called `labels`, which contains all of the *new* sonar integer labels you just created. **Display both of these new DataFrames to make sure they look correct.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      1\n",
      "1      1\n",
      "2      1\n",
      "3      1\n",
      "4      1\n",
      "      ..\n",
      "203    0\n",
      "204    0\n",
      "205    0\n",
      "206    0\n",
      "207    0\n",
      "Name: Class, Length: 208, dtype: int64\n",
      "     attribute_1  attribute_2  attribute_3  attribute_4  attribute_5  \\\n",
      "0         0.0200       0.0371       0.0428       0.0207       0.0954   \n",
      "1         0.0453       0.0523       0.0843       0.0689       0.1183   \n",
      "2         0.0262       0.0582       0.1099       0.1083       0.0974   \n",
      "3         0.0100       0.0171       0.0623       0.0205       0.0205   \n",
      "4         0.0762       0.0666       0.0481       0.0394       0.0590   \n",
      "..           ...          ...          ...          ...          ...   \n",
      "203       0.0187       0.0346       0.0168       0.0177       0.0393   \n",
      "204       0.0323       0.0101       0.0298       0.0564       0.0760   \n",
      "205       0.0522       0.0437       0.0180       0.0292       0.0351   \n",
      "206       0.0303       0.0353       0.0490       0.0608       0.0167   \n",
      "207       0.0260       0.0363       0.0136       0.0272       0.0214   \n",
      "\n",
      "     attribute_6  attribute_7  attribute_8  attribute_9  attribute_10  ...  \\\n",
      "0         0.0986       0.1539       0.1601       0.3109        0.2111  ...   \n",
      "1         0.2583       0.2156       0.3481       0.3337        0.2872  ...   \n",
      "2         0.2280       0.2431       0.3771       0.5598        0.6194  ...   \n",
      "3         0.0368       0.1098       0.1276       0.0598        0.1264  ...   \n",
      "4         0.0649       0.1209       0.2467       0.3564        0.4459  ...   \n",
      "..           ...          ...          ...          ...           ...  ...   \n",
      "203       0.1630       0.2028       0.1694       0.2328        0.2684  ...   \n",
      "204       0.0958       0.0990       0.1018       0.1030        0.2154  ...   \n",
      "205       0.1171       0.1257       0.1178       0.1258        0.2529  ...   \n",
      "206       0.1354       0.1465       0.1123       0.1945        0.2354  ...   \n",
      "207       0.0338       0.0655       0.1400       0.1843        0.2354  ...   \n",
      "\n",
      "     attribute_51  attribute_52  attribute_53  attribute_54  attribute_55  \\\n",
      "0          0.0232        0.0027        0.0065        0.0159        0.0072   \n",
      "1          0.0125        0.0084        0.0089        0.0048        0.0094   \n",
      "2          0.0033        0.0232        0.0166        0.0095        0.0180   \n",
      "3          0.0241        0.0121        0.0036        0.0150        0.0085   \n",
      "4          0.0156        0.0031        0.0054        0.0105        0.0110   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "203        0.0203        0.0116        0.0098        0.0199        0.0033   \n",
      "204        0.0051        0.0061        0.0093        0.0135        0.0063   \n",
      "205        0.0155        0.0160        0.0029        0.0051        0.0062   \n",
      "206        0.0042        0.0086        0.0046        0.0126        0.0036   \n",
      "207        0.0181        0.0146        0.0129        0.0047        0.0039   \n",
      "\n",
      "     attribute_56  attribute_57  attribute_58  attribute_59  attribute_60  \n",
      "0          0.0167        0.0180        0.0084        0.0090        0.0032  \n",
      "1          0.0191        0.0140        0.0049        0.0052        0.0044  \n",
      "2          0.0244        0.0316        0.0164        0.0095        0.0078  \n",
      "3          0.0073        0.0050        0.0044        0.0040        0.0117  \n",
      "4          0.0015        0.0072        0.0048        0.0107        0.0094  \n",
      "..            ...           ...           ...           ...           ...  \n",
      "203        0.0101        0.0065        0.0115        0.0193        0.0157  \n",
      "204        0.0063        0.0034        0.0032        0.0062        0.0067  \n",
      "205        0.0089        0.0140        0.0138        0.0077        0.0031  \n",
      "206        0.0035        0.0034        0.0079        0.0036        0.0048  \n",
      "207        0.0061        0.0040        0.0036        0.0061        0.0115  \n",
      "\n",
      "[208 rows x 60 columns]\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "labels1 = sonar['Class'].replace('Rock',1)\n",
    "labels = labels1.replace('Mine',0)\n",
    "print(labels)\n",
    "features = sonar.iloc[: , :-1]\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 2.1 (1 point):** How balanced is your set of sonar classes? Does it matter for the set of classes to be balanced? Why or why not? (You might need to write a bit of code to figure out how balanced your set of sonar classes is.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> It is a pretty balanced dataset. Rocks(1) have a slightly lower amount of entries at 97, while Mines(0) has 111. Rocks takes up 46.6% of the data, while Mines takes up 53.4% of the data. both are decently close to 50%, so it is safe to say the data is pretty balanced. However; it is not perfectly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of Rocks is: 97\n"
     ]
    }
   ],
   "source": [
    "mask = sonar['Class'] == 1\n",
    "count = len(sonar[mask])\n",
    "\n",
    "print(f\"The number of Rocks is: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of Mines is: 111\n"
     ]
    }
   ],
   "source": [
    "mask = sonar['Class'] == 0\n",
    "count = len(sonar[mask])\n",
    "\n",
    "print(f\"The number of Mines is: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository `hw04_branch` using the commit message \"Committing Part 2\", and push the changes to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Building an SVC model (4 points)\n",
    "\n",
    "Now, to tackle this classification problem, we will use a support vector machine just like we've done previously (e.g. in the **Day 20 and Day 21 assignments**). Of course, we could easily replace this with any `sklearn` classifier we choose, but for now we will just use an SVC with a linear kernel.\n",
    "\n",
    "### 3.1 Splitting the data\n",
    "\n",
    "But first, we need to split our data into training and testing data!\n",
    "\n",
    "**&#9989; Task 3.1 (1 point):** Split your data into a training and testing set with a training set representing 75% of your data. For reproducibility , set the `random_state` argument to `8675309`. Print the lengths to show you have the right number of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n",
      "52\n",
      "156\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, train_size=0.75, random_state=8675309)\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Modeling the data and evaluating the fit\n",
    "\n",
    "As you have done this a number of times at this point, we ask you to do most of the analysis for this problem in one cell.\n",
    "\n",
    "**&#9989; Task 3.2 (2 points):** Build a **linear** kernel SVC model with `C=1.0`, fit it to the training set, and use the test features to predict the outcomes. Evaluate the fit using the **confusion matrix** and **classification report**.\n",
    "\n",
    "**First Note:** Double-check the documentation on the confusion matrix because the way `sklearn` outputs false positives and false negatives may be different from what most images on the web indicate.\n",
    "\n",
    "**Second Note:** You should not be performing a \"grid search\" for this model. We're just trying to get a baseline for how well the model performs, but fitting a single SVC model using the `SVC` class from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8461538461538461\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.90      0.75      0.82        24\n",
      "           0       0.81      0.93      0.87        28\n",
      "\n",
      "    accuracy                           0.85        52\n",
      "   macro avg       0.86      0.84      0.84        52\n",
      "weighted avg       0.85      0.85      0.84        52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "from sklearn import metrics\n",
    "model = SVC(C = 1.0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "cm_display.plot()\n",
    "plt.show()\n",
    "print(sklearn.metrics.classification_report(y_test, y_pred, labels = [1,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 3.1 (1 point):** How accurate is your model? What evidence are you using to determine that? How many false positives and false negatives does it predict for each class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> The model is around 85% accurate. I used the accuracy_score function to determine that. It is also evident in the accuracy and weighted avg in the classification report. There were 2 false positives and 6 false negatives for Rock, and 2 false negatives with 6 falso positives for Mine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository `hw04_branch` using the commit message \"Committing Part 3\", and push the changes to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Finding and using the best hyperparameters (8 points)\n",
    "\n",
    "At this point, we have fit one model and determined it's performance, but is it the best model? We can use `GridSearchCV` to find the best model (given our choices of parameters). Once we do that, we will use that \"best\" model for making predictions. This is similar to what we did when working with the \"digits\" data and the \"faces\" data in the **Day 21 and Day 22 assignments**.\n",
    "\n",
    "**Note:** you would typically rerun this grid search in a production environment to continue to verify the best model, but we are not for the sake of speed.\n",
    "\n",
    "### 4.1 Performing a grid search\n",
    "\n",
    "**&#9989; Task 4.1 (4 points):** Using the following parameters `C` = `0.1`, `1.0`, `10.0`, `100.0`, `1000.0`, `1e4` and `gamma` = `1e-4`, `1e-3`, `0.01`, `0.1`, `1.0`, `10.0` for both a `linear` and `rbf` kernel use `GridSearchCV` with the `SVC()` model to find the best fit parameters. Once, you're run the grid search, print the \"best params\" that the grid search found (*hint*: there's an attribute associated with the GridSearchCV object that stores this information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  param_C  \\\n",
      "0        0.005728      0.000249         0.002946        0.000421      0.1   \n",
      "1        0.004700      0.000378         0.002572        0.000240      0.1   \n",
      "2        0.005048      0.000932         0.002441        0.000477      0.1   \n",
      "3        0.004002      0.000582         0.002409        0.000406      0.1   \n",
      "4        0.005363      0.000375         0.003087        0.000387      0.1   \n",
      "5        0.004934      0.001112         0.002859        0.000824      0.1   \n",
      "6        0.004008      0.000301         0.002088        0.000190      0.1   \n",
      "7        0.003848      0.000315         0.002406        0.000465      0.1   \n",
      "8        0.004636      0.000514         0.002367        0.000515      1.0   \n",
      "9        0.003783      0.000681         0.002107        0.000399      1.0   \n",
      "10       0.004167      0.000638         0.002596        0.000408      1.0   \n",
      "11       0.003707      0.000668         0.001715        0.000091      1.0   \n",
      "12       0.005613      0.001225         0.003039        0.000593      1.0   \n",
      "13       0.004232      0.000593         0.002356        0.000457      1.0   \n",
      "14       0.005179      0.000717         0.002903        0.000392      1.0   \n",
      "15       0.004024      0.000607         0.002049        0.000381      1.0   \n",
      "16       0.004575      0.000949         0.002608        0.000582     10.0   \n",
      "17       0.004575      0.000495         0.002254        0.000470     10.0   \n",
      "18       0.005335      0.000142         0.002806        0.000438     10.0   \n",
      "19       0.004413      0.000549         0.002211        0.000451     10.0   \n",
      "20       0.004641      0.000266         0.002797        0.000533     10.0   \n",
      "21       0.005142      0.000516         0.001940        0.000396     10.0   \n",
      "22       0.004137      0.000529         0.002497        0.000588     10.0   \n",
      "23       0.005475      0.000458         0.002438        0.000283     10.0   \n",
      "24       0.004494      0.000955         0.002430        0.000562    100.0   \n",
      "25       0.014566      0.005427         0.002923        0.000252    100.0   \n",
      "26       0.005351      0.001545         0.002345        0.000316    100.0   \n",
      "27       0.015373      0.005292         0.002735        0.000242    100.0   \n",
      "28       0.004491      0.000666         0.002347        0.000412    100.0   \n",
      "29       0.015193      0.005806         0.003404        0.001887    100.0   \n",
      "30       0.004752      0.000891         0.002160        0.000389    100.0   \n",
      "31       0.013610      0.004632         0.002251        0.000350    100.0   \n",
      "32       0.004453      0.000946         0.002416        0.000410   1000.0   \n",
      "33       0.067347      0.041518         0.001984        0.000187   1000.0   \n",
      "34       0.004715      0.000413         0.002448        0.000342   1000.0   \n",
      "35       0.058565      0.037055         0.001881        0.000179   1000.0   \n",
      "36       0.005891      0.000711         0.002094        0.000285   1000.0   \n",
      "37       0.066761      0.039095         0.002142        0.000128   1000.0   \n",
      "38       0.005849      0.000456         0.002556        0.000082   1000.0   \n",
      "39       0.061787      0.034369         0.002042        0.000184   1000.0   \n",
      "40       0.004204      0.000808         0.002240        0.000422  10000.0   \n",
      "41       0.201116      0.191943         0.001996        0.000352  10000.0   \n",
      "42       0.005871      0.000660         0.002342        0.000373  10000.0   \n",
      "43       0.209948      0.186245         0.002401        0.000535  10000.0   \n",
      "44       0.008294      0.001809         0.002221        0.000325  10000.0   \n",
      "45       0.205628      0.201906         0.002135        0.000310  10000.0   \n",
      "46       0.005017      0.000658         0.002608        0.000391  10000.0   \n",
      "47       0.198213      0.191186         0.002014        0.000299  10000.0   \n",
      "\n",
      "   param_gamma param_kernel  \\\n",
      "0       0.0001          rbf   \n",
      "1       0.0001       linear   \n",
      "2        0.001          rbf   \n",
      "3        0.001       linear   \n",
      "4         0.01          rbf   \n",
      "5         0.01       linear   \n",
      "6          0.1          rbf   \n",
      "7          0.1       linear   \n",
      "8       0.0001          rbf   \n",
      "9       0.0001       linear   \n",
      "10       0.001          rbf   \n",
      "11       0.001       linear   \n",
      "12        0.01          rbf   \n",
      "13        0.01       linear   \n",
      "14         0.1          rbf   \n",
      "15         0.1       linear   \n",
      "16      0.0001          rbf   \n",
      "17      0.0001       linear   \n",
      "18       0.001          rbf   \n",
      "19       0.001       linear   \n",
      "20        0.01          rbf   \n",
      "21        0.01       linear   \n",
      "22         0.1          rbf   \n",
      "23         0.1       linear   \n",
      "24      0.0001          rbf   \n",
      "25      0.0001       linear   \n",
      "26       0.001          rbf   \n",
      "27       0.001       linear   \n",
      "28        0.01          rbf   \n",
      "29        0.01       linear   \n",
      "30         0.1          rbf   \n",
      "31         0.1       linear   \n",
      "32      0.0001          rbf   \n",
      "33      0.0001       linear   \n",
      "34       0.001          rbf   \n",
      "35       0.001       linear   \n",
      "36        0.01          rbf   \n",
      "37        0.01       linear   \n",
      "38         0.1          rbf   \n",
      "39         0.1       linear   \n",
      "40      0.0001          rbf   \n",
      "41      0.0001       linear   \n",
      "42       0.001          rbf   \n",
      "43       0.001       linear   \n",
      "44        0.01          rbf   \n",
      "45        0.01       linear   \n",
      "46         0.1          rbf   \n",
      "47         0.1       linear   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0        {'C': 0.1, 'gamma': 0.0001, 'kernel': 'rbf'}           0.523810   \n",
      "1     {'C': 0.1, 'gamma': 0.0001, 'kernel': 'linear'}           0.404762   \n",
      "2         {'C': 0.1, 'gamma': 0.001, 'kernel': 'rbf'}           0.523810   \n",
      "3      {'C': 0.1, 'gamma': 0.001, 'kernel': 'linear'}           0.404762   \n",
      "4          {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}           0.523810   \n",
      "5       {'C': 0.1, 'gamma': 0.01, 'kernel': 'linear'}           0.404762   \n",
      "6           {'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf'}           0.523810   \n",
      "7        {'C': 0.1, 'gamma': 0.1, 'kernel': 'linear'}           0.404762   \n",
      "8        {'C': 1.0, 'gamma': 0.0001, 'kernel': 'rbf'}           0.523810   \n",
      "9     {'C': 1.0, 'gamma': 0.0001, 'kernel': 'linear'}           0.428571   \n",
      "10        {'C': 1.0, 'gamma': 0.001, 'kernel': 'rbf'}           0.523810   \n",
      "11     {'C': 1.0, 'gamma': 0.001, 'kernel': 'linear'}           0.428571   \n",
      "12         {'C': 1.0, 'gamma': 0.01, 'kernel': 'rbf'}           0.523810   \n",
      "13      {'C': 1.0, 'gamma': 0.01, 'kernel': 'linear'}           0.428571   \n",
      "14          {'C': 1.0, 'gamma': 0.1, 'kernel': 'rbf'}           0.452381   \n",
      "15       {'C': 1.0, 'gamma': 0.1, 'kernel': 'linear'}           0.428571   \n",
      "16      {'C': 10.0, 'gamma': 0.0001, 'kernel': 'rbf'}           0.523810   \n",
      "17   {'C': 10.0, 'gamma': 0.0001, 'kernel': 'linear'}           0.452381   \n",
      "18       {'C': 10.0, 'gamma': 0.001, 'kernel': 'rbf'}           0.523810   \n",
      "19    {'C': 10.0, 'gamma': 0.001, 'kernel': 'linear'}           0.452381   \n",
      "20        {'C': 10.0, 'gamma': 0.01, 'kernel': 'rbf'}           0.404762   \n",
      "21     {'C': 10.0, 'gamma': 0.01, 'kernel': 'linear'}           0.452381   \n",
      "22         {'C': 10.0, 'gamma': 0.1, 'kernel': 'rbf'}           0.500000   \n",
      "23      {'C': 10.0, 'gamma': 0.1, 'kernel': 'linear'}           0.452381   \n",
      "24     {'C': 100.0, 'gamma': 0.0001, 'kernel': 'rbf'}           0.523810   \n",
      "25  {'C': 100.0, 'gamma': 0.0001, 'kernel': 'linear'}           0.452381   \n",
      "26      {'C': 100.0, 'gamma': 0.001, 'kernel': 'rbf'}           0.404762   \n",
      "27   {'C': 100.0, 'gamma': 0.001, 'kernel': 'linear'}           0.452381   \n",
      "28       {'C': 100.0, 'gamma': 0.01, 'kernel': 'rbf'}           0.428571   \n",
      "29    {'C': 100.0, 'gamma': 0.01, 'kernel': 'linear'}           0.452381   \n",
      "30        {'C': 100.0, 'gamma': 0.1, 'kernel': 'rbf'}           0.500000   \n",
      "31     {'C': 100.0, 'gamma': 0.1, 'kernel': 'linear'}           0.452381   \n",
      "32    {'C': 1000.0, 'gamma': 0.0001, 'kernel': 'rbf'}           0.404762   \n",
      "33  {'C': 1000.0, 'gamma': 0.0001, 'kernel': 'line...           0.428571   \n",
      "34     {'C': 1000.0, 'gamma': 0.001, 'kernel': 'rbf'}           0.357143   \n",
      "35  {'C': 1000.0, 'gamma': 0.001, 'kernel': 'linear'}           0.428571   \n",
      "36      {'C': 1000.0, 'gamma': 0.01, 'kernel': 'rbf'}           0.500000   \n",
      "37   {'C': 1000.0, 'gamma': 0.01, 'kernel': 'linear'}           0.428571   \n",
      "38       {'C': 1000.0, 'gamma': 0.1, 'kernel': 'rbf'}           0.500000   \n",
      "39    {'C': 1000.0, 'gamma': 0.1, 'kernel': 'linear'}           0.428571   \n",
      "40   {'C': 10000.0, 'gamma': 0.0001, 'kernel': 'rbf'}           0.357143   \n",
      "41  {'C': 10000.0, 'gamma': 0.0001, 'kernel': 'lin...           0.428571   \n",
      "42    {'C': 10000.0, 'gamma': 0.001, 'kernel': 'rbf'}           0.428571   \n",
      "43  {'C': 10000.0, 'gamma': 0.001, 'kernel': 'line...           0.428571   \n",
      "44     {'C': 10000.0, 'gamma': 0.01, 'kernel': 'rbf'}           0.523810   \n",
      "45  {'C': 10000.0, 'gamma': 0.01, 'kernel': 'linear'}           0.428571   \n",
      "46      {'C': 10000.0, 'gamma': 0.1, 'kernel': 'rbf'}           0.500000   \n",
      "47   {'C': 10000.0, 'gamma': 0.1, 'kernel': 'linear'}           0.428571   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  \\\n",
      "0            0.523810           0.547619           0.536585   \n",
      "1            0.738095           0.595238           0.780488   \n",
      "2            0.523810           0.547619           0.536585   \n",
      "3            0.738095           0.595238           0.780488   \n",
      "4            0.523810           0.547619           0.536585   \n",
      "5            0.738095           0.595238           0.780488   \n",
      "6            0.523810           0.547619           0.536585   \n",
      "7            0.738095           0.595238           0.780488   \n",
      "8            0.523810           0.547619           0.536585   \n",
      "9            0.714286           0.666667           0.829268   \n",
      "10           0.523810           0.547619           0.536585   \n",
      "11           0.714286           0.666667           0.829268   \n",
      "12           0.523810           0.595238           0.536585   \n",
      "13           0.714286           0.666667           0.829268   \n",
      "14           0.761905           0.523810           0.780488   \n",
      "15           0.714286           0.666667           0.829268   \n",
      "16           0.523810           0.547619           0.536585   \n",
      "17           0.690476           0.690476           0.829268   \n",
      "18           0.523810           0.595238           0.536585   \n",
      "19           0.690476           0.690476           0.829268   \n",
      "20           0.714286           0.547619           0.780488   \n",
      "21           0.690476           0.690476           0.829268   \n",
      "22           0.690476           0.547619           0.707317   \n",
      "23           0.690476           0.690476           0.829268   \n",
      "24           0.523810           0.595238           0.536585   \n",
      "25           0.666667           0.690476           0.780488   \n",
      "26           0.714286           0.547619           0.780488   \n",
      "27           0.666667           0.690476           0.780488   \n",
      "28           0.714286           0.642857           0.804878   \n",
      "29           0.666667           0.690476           0.780488   \n",
      "30           0.690476           0.523810           0.731707   \n",
      "31           0.666667           0.690476           0.780488   \n",
      "32           0.714286           0.547619           0.780488   \n",
      "33           0.619048           0.690476           0.878049   \n",
      "34           0.714286           0.642857           0.804878   \n",
      "35           0.619048           0.690476           0.878049   \n",
      "36           0.785714           0.595238           0.780488   \n",
      "37           0.619048           0.690476           0.878049   \n",
      "38           0.714286           0.523810           0.731707   \n",
      "39           0.619048           0.690476           0.878049   \n",
      "40           0.714286           0.666667           0.804878   \n",
      "41           0.571429           0.642857           0.878049   \n",
      "42           0.690476           0.619048           0.756098   \n",
      "43           0.571429           0.642857           0.878049   \n",
      "44           0.738095           0.595238           0.756098   \n",
      "45           0.571429           0.642857           0.878049   \n",
      "46           0.714286           0.523810           0.731707   \n",
      "47           0.571429           0.642857           0.878049   \n",
      "\n",
      "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
      "0            0.536585         0.533682        0.009011               42  \n",
      "1            0.341463         0.572009        0.174763               32  \n",
      "2            0.536585         0.533682        0.009011               42  \n",
      "3            0.341463         0.572009        0.174763               32  \n",
      "4            0.536585         0.533682        0.009011               42  \n",
      "5            0.341463         0.572009        0.174763               32  \n",
      "6            0.536585         0.533682        0.009011               42  \n",
      "7            0.341463         0.572009        0.174763               32  \n",
      "8            0.536585         0.533682        0.009011               42  \n",
      "9            0.487805         0.625319        0.147540               13  \n",
      "10           0.536585         0.533682        0.009011               42  \n",
      "11           0.487805         0.625319        0.147540               13  \n",
      "12           0.560976         0.548084        0.027204               41  \n",
      "13           0.487805         0.625319        0.147540               13  \n",
      "14           0.365854         0.576887        0.166457               31  \n",
      "15           0.487805         0.625319        0.147540               13  \n",
      "16           0.536585         0.533682        0.009011               42  \n",
      "17           0.634146         0.659350        0.121886                1  \n",
      "18           0.585366         0.552962        0.031001               36  \n",
      "19           0.634146         0.659350        0.121886                1  \n",
      "20           0.317073         0.552846        0.176311               38  \n",
      "21           0.634146         0.659350        0.121886                1  \n",
      "22           0.609756         0.611034        0.079916               20  \n",
      "23           0.634146         0.659350        0.121886                1  \n",
      "24           0.585366         0.552962        0.031001               36  \n",
      "25           0.536585         0.625319        0.116459                9  \n",
      "26           0.317073         0.552846        0.176311               38  \n",
      "27           0.536585         0.625319        0.116459                9  \n",
      "28           0.536585         0.625436        0.131945                8  \n",
      "29           0.536585         0.625319        0.116459                9  \n",
      "30           0.609756         0.611150        0.090345               18  \n",
      "31           0.536585         0.625319        0.116459                9  \n",
      "32           0.317073         0.552846        0.176311               38  \n",
      "33           0.439024         0.611034        0.167662               20  \n",
      "34           0.536585         0.611150        0.154436               18  \n",
      "35           0.439024         0.611034        0.167662               20  \n",
      "36           0.585366         0.649361        0.114123                5  \n",
      "37           0.439024         0.611034        0.167662               20  \n",
      "38           0.585366         0.611034        0.095727               20  \n",
      "39           0.439024         0.611034        0.167662               20  \n",
      "40           0.560976         0.620790        0.153507               17  \n",
      "41           0.512195         0.606620        0.152868               27  \n",
      "42           0.682927         0.635424        0.112166                6  \n",
      "43           0.512195         0.606620        0.152868               27  \n",
      "44           0.560976         0.634843        0.094570                7  \n",
      "45           0.512195         0.606620        0.152868               27  \n",
      "46           0.585366         0.611034        0.095727               20  \n",
      "47           0.512195         0.606620        0.152868               27  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "    X, y = features, labels\n",
    "\n",
    "    param_grid = {'C' : [0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0], 'gamma' : [1e-4, 1e-3, 1e-2, 1e-1], 'kernel': ('rbf', 'linear')}\n",
    "    classifier = SVC()\n",
    "    grid_search = GridSearchCV(estimator=classifier, param_grid=param_grid, scoring='accuracy', n_jobs=-1, refit = True, verbose=0)\n",
    "    grid_search.fit(X,y)\n",
    "    print(pd.DataFrame(grid_search.cv_results_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best parameters were when C = 10 in linear kernel. Gamma seemingly had little to no affect. 17,19,21,23 all were rank 1 in median score. These had C=10 and every variation of gamma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.1 (1 point):** How do the \"best params\" results of the grid search compare to what you used in Part 3? Did the hyper parameter(s) change? What kernel did the grid search determine was the best option? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> These hyper parameters are different from part 3. C was 1.0 in part 3, but grid search determined that 10.0 was the ideal parameter. It also determined that linear was better than rbf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Evaluating the best fit model\n",
    "\n",
    "Now that we have found the \"best params\", let's determine how good the fit is.\n",
    "\n",
    "**&#9989; Task 4.2 (2 points):** Use the test features to predict the outcomes for the best model. Evaluate the fit using the **confusion matrix** and **classification report**.\n",
    "\n",
    "**Note:** Double-check the documentation on the confusion matrix because the way `sklearn` outputs false positives and false negatives may be different from what most images on the web indicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8846153846153846\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.88      0.88      0.88        24\n",
      "           0       0.89      0.89      0.89        28\n",
      "\n",
      "    accuracy                           0.88        52\n",
      "   macro avg       0.88      0.88      0.88        52\n",
      "weighted avg       0.88      0.88      0.88        52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "model = SVC(C = 10.0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "cm_display.plot()\n",
    "plt.show()\n",
    "print(sklearn.metrics.classification_report(y_test, y_pred, labels = [1,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.2 (1 point):** How accurate is this \"best\" model? What evidence are you using to determine that? How many false positives and false negatives does it predict?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> This best model is 88% accurate. I am using the accuracy and weighted avg in the classification report as well as the accuracy_score function. It produces 3 false positives and negatives for both. it is slightly worse for predicting 0's, but significantly better at predicting 1's than at C = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository `hw04_branch` using the commit message \"Committing Part 4\", and push the changes to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Using Principal Components (10 points)\n",
    "\n",
    "The full model uses all 60 sonar features to predict the results and you likely found that the model is decently accurate using all 60 features, but not perfect. **Could we get the same level of accuracy (or better) using fewer features?** When datasets start to get very large and complex, applying some sort of **feature reduction** method can reduce the computational resources needed to train the model and, in some case actually improve the accuracy.\n",
    "\n",
    "When performing feature reduction, one could simply try to identify which features seem most important and drop the ones that aren't, but performing a Principal Component Analysis (PCA) to determine the features that contribute the most to the model (through their accounted variance) can be more effective. We did this to improve our classification with the \"faces\" dataset in the **Day 22 assignment**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Running a Principle Component Analysis (PCA)\n",
    "\n",
    "Since we have 60 total features to start with, let's see how well we can do if only use 5% as many features reduce the feature count to **3** principle components. We'll see how well we can predict the classes of the sonar dataset with just **3 features**!\n",
    "\n",
    "**&#9989; Task 5.1 (3 points):**  Using `PCA()` and the associated `fit()` method, run a principle component analysis on your training features using 3 components. Transform both the test and training features using the result of your PCA. Print the `explained_variance_ratio_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     principal component 1  principal component 2  principal component 3  \\\n",
      "0                 1.921168              -1.370893              -1.666476   \n",
      "1                -0.480125               7.586388              -1.275734   \n",
      "2                 3.859228               6.439860              -0.030635   \n",
      "3                 4.597419              -3.104089              -1.785344   \n",
      "4                -0.533868               1.849847              -0.860097   \n",
      "..                     ...                    ...                    ...   \n",
      "203              -1.207653              -0.968174               3.116148   \n",
      "204              -2.971439              -2.753492               2.500966   \n",
      "205              -2.293210              -2.755446               2.388467   \n",
      "206              -3.114464              -1.850550               2.420486   \n",
      "207              -3.238624              -2.277094               1.706128   \n",
      "\n",
      "     Class  \n",
      "0        1  \n",
      "1        1  \n",
      "2        1  \n",
      "3        1  \n",
      "4        1  \n",
      "..     ...  \n",
      "203      0  \n",
      "204      0  \n",
      "205      0  \n",
      "206      0  \n",
      "207      0  \n",
      "\n",
      "[208 rows x 4 columns]\n",
      "[0.20346557 0.18897216 0.08549989]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.47793761535378315"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xfeatures = StandardScaler().fit_transform(features)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "principalComponents = pca.fit_transform(xfeatures)\n",
    "\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2','principal component 3'])\n",
    "finalDf = pd.concat([principalDf, labels], axis = 1)\n",
    "print(finalDf)\n",
    "evr = pca.explained_variance_ratio_\n",
    "print(evr)\n",
    "sum(evr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 5.1 (1 point):** What is the total explained variance ratio captured by this simple 3-component PCA? (e.g. sum up the explained variance from all 3 components) How well do you think a model with this many feature will perform? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> The sum of the explained variance ratio is .4779. A model with this many features will perform below average as the explained variance from these 3 components is under 50%. The models accuracy would probably be around the same as the explained variance, meaning it is not a very accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Fit and Evaluate an SVC model\n",
    "\n",
    "Using the PCA transformed features, we need to train and test a new SVC model. You'll want to perform the `GridSearchCV` again since there may a better choice for the kernel and the hyper-parameters.\n",
    "\n",
    "**&#9989; Task 5.2 (2 points):**  Using the PCA transformed training data, build and train an SVC model using the `GridSearchCV` tool to make sure you're using the best kernel and hyper-parameter combination. Predict the classes using the PCA transformed test data. Evaluate the model using the classification report, and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  param_C  \\\n",
      "0        0.003596      0.000066         0.001700        0.000036      0.1   \n",
      "1        0.003591      0.000575         0.001570        0.000350      0.1   \n",
      "2        0.003574      0.000177         0.001671        0.000019      0.1   \n",
      "3        0.003596      0.000780         0.001724        0.000446      0.1   \n",
      "4        0.006757      0.006165         0.002747        0.000632      0.1   \n",
      "5        0.005252      0.002335         0.002077        0.000218      0.1   \n",
      "6        0.005234      0.000491         0.002858        0.000340      0.1   \n",
      "7        0.004675      0.000985         0.002788        0.001774      0.1   \n",
      "8        0.005082      0.000260         0.002504        0.000379      1.0   \n",
      "9        0.005062      0.000615         0.001864        0.000431      1.0   \n",
      "10       0.004505      0.000182         0.002301        0.000415      1.0   \n",
      "11       0.005626      0.000633         0.002019        0.000198      1.0   \n",
      "12       0.004467      0.000451         0.002485        0.000345      1.0   \n",
      "13       0.005833      0.000414         0.002232        0.000274      1.0   \n",
      "14       0.005027      0.000241         0.002441        0.000166      1.0   \n",
      "15       0.005092      0.000787         0.001985        0.000503      1.0   \n",
      "16       0.004447      0.000235         0.002566        0.000139     10.0   \n",
      "17       0.014185      0.004311         0.001702        0.000307     10.0   \n",
      "18       0.004404      0.000500         0.002252        0.000299     10.0   \n",
      "19       0.017926      0.003202         0.002254        0.000294     10.0   \n",
      "20       0.004213      0.000571         0.002088        0.000264     10.0   \n",
      "21       0.016289      0.004746         0.001873        0.000377     10.0   \n",
      "22       0.004948      0.000730         0.002234        0.000483     10.0   \n",
      "23       0.017921      0.005856         0.003028        0.001856     10.0   \n",
      "24       0.004008      0.000862         0.002026        0.000483    100.0   \n",
      "25       0.097007      0.008917         0.001956        0.000424    100.0   \n",
      "26       0.004242      0.000677         0.002089        0.000431    100.0   \n",
      "27       0.108383      0.018283         0.002165        0.000919    100.0   \n",
      "28       0.004889      0.000571         0.002047        0.000374    100.0   \n",
      "29       0.119181      0.022067         0.002333        0.000227    100.0   \n",
      "30       0.004602      0.000743         0.001901        0.000219    100.0   \n",
      "31       0.116795      0.014222         0.001617        0.000210    100.0   \n",
      "32       0.004392      0.000504         0.001989        0.000293   1000.0   \n",
      "33       0.821645      0.219634         0.002672        0.000941   1000.0   \n",
      "34       0.006146      0.000400         0.001837        0.000033   1000.0   \n",
      "35       0.774640      0.182280         0.001798        0.000243   1000.0   \n",
      "36       0.005186      0.001293         0.001591        0.000316   1000.0   \n",
      "37       0.819843      0.237723         0.002698        0.002173   1000.0   \n",
      "38       0.005463      0.000618         0.002427        0.000177   1000.0   \n",
      "39       0.839697      0.211286         0.001794        0.000191   1000.0   \n",
      "40       0.006111      0.000570         0.001778        0.000205  10000.0   \n",
      "41       8.977683      3.458497         0.003302        0.001436  10000.0   \n",
      "42       0.020404      0.002925         0.002176        0.000303  10000.0   \n",
      "43       9.042138      3.439044         0.002749        0.001448  10000.0   \n",
      "44       0.008154      0.001314         0.002017        0.000133  10000.0   \n",
      "45       8.955255      3.469428         0.002751        0.001378  10000.0   \n",
      "46       0.004439      0.000545         0.001716        0.000216  10000.0   \n",
      "47       8.996176      3.446437         0.003441        0.001393  10000.0   \n",
      "\n",
      "   param_gamma param_kernel  \\\n",
      "0       0.0001          rbf   \n",
      "1       0.0001       linear   \n",
      "2        0.001          rbf   \n",
      "3        0.001       linear   \n",
      "4         0.01          rbf   \n",
      "5         0.01       linear   \n",
      "6          0.1          rbf   \n",
      "7          0.1       linear   \n",
      "8       0.0001          rbf   \n",
      "9       0.0001       linear   \n",
      "10       0.001          rbf   \n",
      "11       0.001       linear   \n",
      "12        0.01          rbf   \n",
      "13        0.01       linear   \n",
      "14         0.1          rbf   \n",
      "15         0.1       linear   \n",
      "16      0.0001          rbf   \n",
      "17      0.0001       linear   \n",
      "18       0.001          rbf   \n",
      "19       0.001       linear   \n",
      "20        0.01          rbf   \n",
      "21        0.01       linear   \n",
      "22         0.1          rbf   \n",
      "23         0.1       linear   \n",
      "24      0.0001          rbf   \n",
      "25      0.0001       linear   \n",
      "26       0.001          rbf   \n",
      "27       0.001       linear   \n",
      "28        0.01          rbf   \n",
      "29        0.01       linear   \n",
      "30         0.1          rbf   \n",
      "31         0.1       linear   \n",
      "32      0.0001          rbf   \n",
      "33      0.0001       linear   \n",
      "34       0.001          rbf   \n",
      "35       0.001       linear   \n",
      "36        0.01          rbf   \n",
      "37        0.01       linear   \n",
      "38         0.1          rbf   \n",
      "39         0.1       linear   \n",
      "40      0.0001          rbf   \n",
      "41      0.0001       linear   \n",
      "42       0.001          rbf   \n",
      "43       0.001       linear   \n",
      "44        0.01          rbf   \n",
      "45        0.01       linear   \n",
      "46         0.1          rbf   \n",
      "47         0.1       linear   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0        {'C': 0.1, 'gamma': 0.0001, 'kernel': 'rbf'}           0.523810   \n",
      "1     {'C': 0.1, 'gamma': 0.0001, 'kernel': 'linear'}           0.500000   \n",
      "2         {'C': 0.1, 'gamma': 0.001, 'kernel': 'rbf'}           0.523810   \n",
      "3      {'C': 0.1, 'gamma': 0.001, 'kernel': 'linear'}           0.500000   \n",
      "4          {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}           0.690476   \n",
      "5       {'C': 0.1, 'gamma': 0.01, 'kernel': 'linear'}           0.500000   \n",
      "6           {'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf'}           0.571429   \n",
      "7        {'C': 0.1, 'gamma': 0.1, 'kernel': 'linear'}           0.500000   \n",
      "8        {'C': 1.0, 'gamma': 0.0001, 'kernel': 'rbf'}           0.523810   \n",
      "9     {'C': 1.0, 'gamma': 0.0001, 'kernel': 'linear'}           0.500000   \n",
      "10        {'C': 1.0, 'gamma': 0.001, 'kernel': 'rbf'}           0.523810   \n",
      "11     {'C': 1.0, 'gamma': 0.001, 'kernel': 'linear'}           0.500000   \n",
      "12         {'C': 1.0, 'gamma': 0.01, 'kernel': 'rbf'}           0.547619   \n",
      "13      {'C': 1.0, 'gamma': 0.01, 'kernel': 'linear'}           0.500000   \n",
      "14          {'C': 1.0, 'gamma': 0.1, 'kernel': 'rbf'}           0.666667   \n",
      "15       {'C': 1.0, 'gamma': 0.1, 'kernel': 'linear'}           0.500000   \n",
      "16      {'C': 10.0, 'gamma': 0.0001, 'kernel': 'rbf'}           0.476190   \n",
      "17   {'C': 10.0, 'gamma': 0.0001, 'kernel': 'linear'}           0.500000   \n",
      "18       {'C': 10.0, 'gamma': 0.001, 'kernel': 'rbf'}           0.523810   \n",
      "19    {'C': 10.0, 'gamma': 0.001, 'kernel': 'linear'}           0.500000   \n",
      "20        {'C': 10.0, 'gamma': 0.01, 'kernel': 'rbf'}           0.571429   \n",
      "21     {'C': 10.0, 'gamma': 0.01, 'kernel': 'linear'}           0.500000   \n",
      "22         {'C': 10.0, 'gamma': 0.1, 'kernel': 'rbf'}           0.595238   \n",
      "23      {'C': 10.0, 'gamma': 0.1, 'kernel': 'linear'}           0.500000   \n",
      "24     {'C': 100.0, 'gamma': 0.0001, 'kernel': 'rbf'}           0.500000   \n",
      "25  {'C': 100.0, 'gamma': 0.0001, 'kernel': 'linear'}           0.500000   \n",
      "26      {'C': 100.0, 'gamma': 0.001, 'kernel': 'rbf'}           0.547619   \n",
      "27   {'C': 100.0, 'gamma': 0.001, 'kernel': 'linear'}           0.500000   \n",
      "28       {'C': 100.0, 'gamma': 0.01, 'kernel': 'rbf'}           0.547619   \n",
      "29    {'C': 100.0, 'gamma': 0.01, 'kernel': 'linear'}           0.500000   \n",
      "30        {'C': 100.0, 'gamma': 0.1, 'kernel': 'rbf'}           0.571429   \n",
      "31     {'C': 100.0, 'gamma': 0.1, 'kernel': 'linear'}           0.500000   \n",
      "32    {'C': 1000.0, 'gamma': 0.0001, 'kernel': 'rbf'}           0.500000   \n",
      "33  {'C': 1000.0, 'gamma': 0.0001, 'kernel': 'line...           0.500000   \n",
      "34     {'C': 1000.0, 'gamma': 0.001, 'kernel': 'rbf'}           0.571429   \n",
      "35  {'C': 1000.0, 'gamma': 0.001, 'kernel': 'linear'}           0.500000   \n",
      "36      {'C': 1000.0, 'gamma': 0.01, 'kernel': 'rbf'}           0.595238   \n",
      "37   {'C': 1000.0, 'gamma': 0.01, 'kernel': 'linear'}           0.500000   \n",
      "38       {'C': 1000.0, 'gamma': 0.1, 'kernel': 'rbf'}           0.571429   \n",
      "39    {'C': 1000.0, 'gamma': 0.1, 'kernel': 'linear'}           0.500000   \n",
      "40   {'C': 10000.0, 'gamma': 0.0001, 'kernel': 'rbf'}           0.547619   \n",
      "41  {'C': 10000.0, 'gamma': 0.0001, 'kernel': 'lin...           0.500000   \n",
      "42    {'C': 10000.0, 'gamma': 0.001, 'kernel': 'rbf'}           0.619048   \n",
      "43  {'C': 10000.0, 'gamma': 0.001, 'kernel': 'line...           0.500000   \n",
      "44     {'C': 10000.0, 'gamma': 0.01, 'kernel': 'rbf'}           0.547619   \n",
      "45  {'C': 10000.0, 'gamma': 0.01, 'kernel': 'linear'}           0.500000   \n",
      "46      {'C': 10000.0, 'gamma': 0.1, 'kernel': 'rbf'}           0.571429   \n",
      "47   {'C': 10000.0, 'gamma': 0.1, 'kernel': 'linear'}           0.500000   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  \\\n",
      "0            0.523810           0.547619           0.536585   \n",
      "1            0.904762           0.785714           0.756098   \n",
      "2            0.523810           0.547619           0.536585   \n",
      "3            0.904762           0.785714           0.756098   \n",
      "4            0.595238           0.690476           0.780488   \n",
      "5            0.904762           0.785714           0.756098   \n",
      "6            0.523810           0.595238           0.536585   \n",
      "7            0.904762           0.785714           0.756098   \n",
      "8            0.523810           0.547619           0.536585   \n",
      "9            0.880952           0.761905           0.731707   \n",
      "10           0.785714           0.785714           0.731707   \n",
      "11           0.880952           0.761905           0.731707   \n",
      "12           0.785714           0.666667           0.658537   \n",
      "13           0.880952           0.761905           0.731707   \n",
      "14           0.809524           0.642857           0.707317   \n",
      "15           0.880952           0.761905           0.731707   \n",
      "16           0.785714           0.785714           0.756098   \n",
      "17           0.880952           0.761905           0.756098   \n",
      "18           0.857143           0.761905           0.756098   \n",
      "19           0.880952           0.761905           0.756098   \n",
      "20           0.785714           0.666667           0.658537   \n",
      "21           0.880952           0.761905           0.756098   \n",
      "22           0.833333           0.666667           0.804878   \n",
      "23           0.880952           0.761905           0.756098   \n",
      "24           0.880952           0.785714           0.756098   \n",
      "25           0.880952           0.761905           0.756098   \n",
      "26           0.809524           0.666667           0.707317   \n",
      "27           0.880952           0.761905           0.756098   \n",
      "28           0.714286           0.619048           0.731707   \n",
      "29           0.880952           0.761905           0.756098   \n",
      "30           0.761905           0.642857           0.804878   \n",
      "31           0.880952           0.761905           0.756098   \n",
      "32           0.833333           0.761905           0.756098   \n",
      "33           0.880952           0.761905           0.756098   \n",
      "34           0.738095           0.690476           0.658537   \n",
      "35           0.880952           0.761905           0.756098   \n",
      "36           0.761905           0.642857           0.756098   \n",
      "37           0.880952           0.761905           0.756098   \n",
      "38           0.761905           0.642857           0.804878   \n",
      "39           0.880952           0.761905           0.756098   \n",
      "40           0.809524           0.642857           0.731707   \n",
      "41           0.880952           0.785714           0.731707   \n",
      "42           0.642857           0.666667           0.682927   \n",
      "43           0.880952           0.785714           0.731707   \n",
      "44           0.785714           0.619048           0.829268   \n",
      "45           0.880952           0.785714           0.731707   \n",
      "46           0.761905           0.642857           0.804878   \n",
      "47           0.880952           0.785714           0.731707   \n",
      "\n",
      "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
      "0            0.536585         0.533682        0.009011               46  \n",
      "1            0.439024         0.677120        0.177714                6  \n",
      "2            0.536585         0.533682        0.009011               46  \n",
      "3            0.439024         0.677120        0.177714                6  \n",
      "4            0.341463         0.619628        0.150921               44  \n",
      "5            0.439024         0.677120        0.177714                6  \n",
      "6            0.536585         0.552729        0.026511               45  \n",
      "7            0.439024         0.677120        0.177714                6  \n",
      "8            0.536585         0.533682        0.009011               46  \n",
      "9            0.414634         0.657840        0.173279               20  \n",
      "10           0.365854         0.638560        0.167046               41  \n",
      "11           0.414634         0.657840        0.173279               20  \n",
      "12           0.463415         0.624390        0.110259               43  \n",
      "13           0.414634         0.657840        0.173279               20  \n",
      "14           0.487805         0.662834        0.104447               15  \n",
      "15           0.414634         0.657840        0.173279               20  \n",
      "16           0.365854         0.633914        0.177622               42  \n",
      "17           0.390244         0.657840        0.182639               20  \n",
      "18           0.439024         0.667596        0.158477               12  \n",
      "19           0.390244         0.657840        0.182639               20  \n",
      "20           0.682927         0.673055        0.068365               11  \n",
      "21           0.390244         0.657840        0.182639               20  \n",
      "22           0.634146         0.706852        0.094832                1  \n",
      "23           0.390244         0.657840        0.182639               20  \n",
      "24           0.414634         0.667480        0.178540               13  \n",
      "25           0.390244         0.657840        0.182639               20  \n",
      "26           0.512195         0.648664        0.108142               39  \n",
      "27           0.390244         0.657840        0.182639               20  \n",
      "28           0.609756         0.644483        0.068871               40  \n",
      "29           0.390244         0.657840        0.182639               20  \n",
      "30           0.634146         0.683043        0.086633                2  \n",
      "31           0.390244         0.657840        0.182639               20  \n",
      "32           0.414634         0.653194        0.164461               38  \n",
      "33           0.390244         0.657840        0.182639               20  \n",
      "34           0.658537         0.663415        0.054440               14  \n",
      "35           0.390244         0.657840        0.182639               20  \n",
      "36           0.634146         0.678049        0.068040                5  \n",
      "37           0.390244         0.657840        0.182639               20  \n",
      "38           0.634146         0.683043        0.086633                2  \n",
      "39           0.390244         0.657840        0.182639               20  \n",
      "40           0.536585         0.653659        0.105312               37  \n",
      "41           0.414634         0.662602        0.176373               16  \n",
      "42           0.658537         0.654007        0.021742               36  \n",
      "43           0.414634         0.662602        0.176373               16  \n",
      "44           0.585366         0.673403        0.112636               10  \n",
      "45           0.414634         0.662602        0.176373               16  \n",
      "46           0.634146         0.683043        0.086633                2  \n",
      "47           0.414634         0.662602        0.176373               16  \n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "if __name__=='__main__':\n",
    "\n",
    "    X, y = principalDf, labels\n",
    "\n",
    "    param_grid = {'C' : [0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0], 'gamma' : [1e-4, 1e-3, 1e-2, 1e-1], 'kernel': ('rbf', 'linear')}\n",
    "    classifier = SVC()\n",
    "    grid_search = GridSearchCV(estimator=classifier, param_grid=param_grid, scoring='accuracy', n_jobs=-1, refit = True, verbose=0)\n",
    "    grid_search.fit(X,y)\n",
    "    print(pd.DataFrame(grid_search.cv_results_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6730769230769231\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.58      0.62        24\n",
      "           0       0.68      0.75      0.71        28\n",
      "\n",
      "    accuracy                           0.67        52\n",
      "   macro avg       0.67      0.67      0.67        52\n",
      "weighted avg       0.67      0.67      0.67        52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(principalDf, labels, train_size=0.75, random_state=8675309)\n",
    "model = SVC(C = 100.0, gamma = .0001, kernel = 'rbf')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "cm_display.plot()\n",
    "plt.show()\n",
    "print(sklearn.metrics.classification_report(y_test, y_pred, labels = [1,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 5.2 (1 point):** How accurate is this model? What evidence are you using to determine that? How many false positives and false negatives does it predict? How does it compare to the full feature model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> It is 67% accurate. Using the accuracy_score function along with the classification report the accuracy was determined. It has 7 false positives and 10 false negatives. It is noticeably less accurate than the best parameters seen in part 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Repeat your analysis with more components\n",
    "\n",
    "You probably found that the model with 3 features didn't actually do too bad, which is great given how few features we're using, but it's still not as good as just using all of the feature. Can we do better?\n",
    "\n",
    "What if we increase the number of principle components to **6** (10% of the original feature count)? What happens now?\n",
    "\n",
    "**&#9989; Task 5.3 (2 points):** Repeat your analysis from 5.1 and 5.2 using **6 components** instead. As part of your analysis, **print the total explained variance ratio for both components as well as the sum of these values**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     principal component 1  principal component 2  principal component 3  \\\n",
      "0                 1.921168              -1.370893              -1.666476   \n",
      "1                -0.480125               7.586388              -1.275734   \n",
      "2                 3.859228               6.439860              -0.030635   \n",
      "3                 4.597419              -3.104089              -1.785344   \n",
      "4                -0.533868               1.849847              -0.860097   \n",
      "..                     ...                    ...                    ...   \n",
      "203              -1.207653              -0.968174               3.116148   \n",
      "204              -2.971439              -2.753492               2.500966   \n",
      "205              -2.293210              -2.755446               2.388467   \n",
      "206              -3.114464              -1.850550               2.420486   \n",
      "207              -3.238624              -2.277094               1.706128   \n",
      "\n",
      "     principal component 4  principal component 5  principal component 6  \\\n",
      "0                 0.837913              -1.057324               1.712504   \n",
      "1                 3.859346               2.121112              -2.186818   \n",
      "2                 5.454599               1.552060               1.181619   \n",
      "3                -1.115908              -2.785528              -2.072673   \n",
      "4                 3.302076               2.808954              -0.783945   \n",
      "..                     ...                    ...                    ...   \n",
      "203              -0.212490               2.306835               1.151569   \n",
      "204               0.478754               2.157206               0.928558   \n",
      "205               0.130616               2.759021               1.049497   \n",
      "206               0.382211               1.658317               0.917819   \n",
      "207              -0.225883               1.126790               0.980035   \n",
      "\n",
      "     Class  \n",
      "0        1  \n",
      "1        1  \n",
      "2        1  \n",
      "3        1  \n",
      "4        1  \n",
      "..     ...  \n",
      "203      0  \n",
      "204      0  \n",
      "205      0  \n",
      "206      0  \n",
      "207      0  \n",
      "\n",
      "[208 rows x 7 columns]\n",
      "[0.20346557 0.18897216 0.08549989 0.0567919  0.0500708  0.04064995]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6254502706438427"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put your code here\n",
    "xfeatures = StandardScaler().fit_transform(features)\n",
    "\n",
    "pca = PCA(n_components=6)\n",
    "\n",
    "principalComponents = pca.fit_transform(xfeatures)\n",
    "\n",
    "principalDf1 = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2','principal component 3','principal component 4','principal component 5','principal component 6'])\n",
    "finalDf = pd.concat([principalDf1, labels], axis = 1)\n",
    "print(finalDf)\n",
    "evr = pca.explained_variance_ratio_\n",
    "print(evr)\n",
    "sum(evr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  param_C  \\\n",
      "0        0.004724      0.001140         0.002349        0.000458      0.1   \n",
      "1        0.004055      0.000449         0.004245        0.003309      0.1   \n",
      "2        0.003839      0.000835         0.002137        0.000431      0.1   \n",
      "3        0.003701      0.000700         0.001547        0.000254      0.1   \n",
      "4        0.003796      0.000323         0.001890        0.000370      0.1   \n",
      "5        0.004125      0.001399         0.001720        0.000228      0.1   \n",
      "6        0.003721      0.000449         0.002017        0.000426      0.1   \n",
      "7        0.004707      0.003264         0.002444        0.001890      0.1   \n",
      "8        0.004459      0.000470         0.002399        0.000405      1.0   \n",
      "9        0.005400      0.000501         0.002098        0.000369      1.0   \n",
      "10       0.004361      0.000549         0.002324        0.000390      1.0   \n",
      "11       0.004667      0.000999         0.001833        0.000412      1.0   \n",
      "12       0.004279      0.000457         0.002145        0.000257      1.0   \n",
      "13       0.004858      0.000736         0.001796        0.000527      1.0   \n",
      "14       0.004146      0.000485         0.002546        0.000607      1.0   \n",
      "15       0.005708      0.000303         0.002406        0.000136      1.0   \n",
      "16       0.003184      0.000336         0.001784        0.000338     10.0   \n",
      "17       0.012462      0.002665         0.001522        0.000129     10.0   \n",
      "18       0.004014      0.000801         0.002200        0.000474     10.0   \n",
      "19       0.014159      0.004212         0.001985        0.000524     10.0   \n",
      "20       0.003692      0.000664         0.002235        0.000416     10.0   \n",
      "21       0.013997      0.002786         0.001738        0.000266     10.0   \n",
      "22       0.004892      0.000702         0.002234        0.000424     10.0   \n",
      "23       0.015769      0.001180         0.002242        0.000536     10.0   \n",
      "24       0.003601      0.000409         0.001875        0.000362    100.0   \n",
      "25       0.094332      0.005563         0.001652        0.000391    100.0   \n",
      "26       0.004336      0.000693         0.002015        0.000360    100.0   \n",
      "27       0.125380      0.028064         0.002165        0.000264    100.0   \n",
      "28       0.004660      0.000466         0.001787        0.000225    100.0   \n",
      "29       0.114467      0.024169         0.001678        0.000188    100.0   \n",
      "30       0.003442      0.000140         0.001599        0.000157    100.0   \n",
      "31       0.121385      0.013353         0.002179        0.000411    100.0   \n",
      "32       0.004352      0.000804         0.002008        0.000310   1000.0   \n",
      "33       0.772678      0.187476         0.004593        0.001271   1000.0   \n",
      "34       0.005940      0.000768         0.002199        0.000238   1000.0   \n",
      "35       0.776402      0.182452         0.002532        0.001126   1000.0   \n",
      "36       0.005303      0.001177         0.001837        0.000556   1000.0   \n",
      "37       0.765587      0.183019         0.002120        0.000565   1000.0   \n",
      "38       0.004789      0.000987         0.002002        0.000386   1000.0   \n",
      "39       0.792705      0.164230         0.001814        0.000227   1000.0   \n",
      "40       0.006301      0.001290         0.001889        0.000461  10000.0   \n",
      "41       8.946140      3.471595         0.002612        0.001168  10000.0   \n",
      "42       0.018597      0.002806         0.002203        0.000579  10000.0   \n",
      "43       8.979427      3.466458         0.003878        0.001448  10000.0   \n",
      "44       0.006917      0.001389         0.001811        0.000225  10000.0   \n",
      "45       8.954420      3.487073         0.003145        0.001365  10000.0   \n",
      "46       0.005135      0.001147         0.002096        0.000426  10000.0   \n",
      "47       9.007385      3.437343         0.002388        0.000823  10000.0   \n",
      "\n",
      "   param_gamma param_kernel  \\\n",
      "0       0.0001          rbf   \n",
      "1       0.0001       linear   \n",
      "2        0.001          rbf   \n",
      "3        0.001       linear   \n",
      "4         0.01          rbf   \n",
      "5         0.01       linear   \n",
      "6          0.1          rbf   \n",
      "7          0.1       linear   \n",
      "8       0.0001          rbf   \n",
      "9       0.0001       linear   \n",
      "10       0.001          rbf   \n",
      "11       0.001       linear   \n",
      "12        0.01          rbf   \n",
      "13        0.01       linear   \n",
      "14         0.1          rbf   \n",
      "15         0.1       linear   \n",
      "16      0.0001          rbf   \n",
      "17      0.0001       linear   \n",
      "18       0.001          rbf   \n",
      "19       0.001       linear   \n",
      "20        0.01          rbf   \n",
      "21        0.01       linear   \n",
      "22         0.1          rbf   \n",
      "23         0.1       linear   \n",
      "24      0.0001          rbf   \n",
      "25      0.0001       linear   \n",
      "26       0.001          rbf   \n",
      "27       0.001       linear   \n",
      "28        0.01          rbf   \n",
      "29        0.01       linear   \n",
      "30         0.1          rbf   \n",
      "31         0.1       linear   \n",
      "32      0.0001          rbf   \n",
      "33      0.0001       linear   \n",
      "34       0.001          rbf   \n",
      "35       0.001       linear   \n",
      "36        0.01          rbf   \n",
      "37        0.01       linear   \n",
      "38         0.1          rbf   \n",
      "39         0.1       linear   \n",
      "40      0.0001          rbf   \n",
      "41      0.0001       linear   \n",
      "42       0.001          rbf   \n",
      "43       0.001       linear   \n",
      "44        0.01          rbf   \n",
      "45        0.01       linear   \n",
      "46         0.1          rbf   \n",
      "47         0.1       linear   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0        {'C': 0.1, 'gamma': 0.0001, 'kernel': 'rbf'}           0.523810   \n",
      "1     {'C': 0.1, 'gamma': 0.0001, 'kernel': 'linear'}           0.500000   \n",
      "2         {'C': 0.1, 'gamma': 0.001, 'kernel': 'rbf'}           0.523810   \n",
      "3      {'C': 0.1, 'gamma': 0.001, 'kernel': 'linear'}           0.500000   \n",
      "4          {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}           0.690476   \n",
      "5       {'C': 0.1, 'gamma': 0.01, 'kernel': 'linear'}           0.500000   \n",
      "6           {'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf'}           0.571429   \n",
      "7        {'C': 0.1, 'gamma': 0.1, 'kernel': 'linear'}           0.500000   \n",
      "8        {'C': 1.0, 'gamma': 0.0001, 'kernel': 'rbf'}           0.523810   \n",
      "9     {'C': 1.0, 'gamma': 0.0001, 'kernel': 'linear'}           0.500000   \n",
      "10        {'C': 1.0, 'gamma': 0.001, 'kernel': 'rbf'}           0.523810   \n",
      "11     {'C': 1.0, 'gamma': 0.001, 'kernel': 'linear'}           0.500000   \n",
      "12         {'C': 1.0, 'gamma': 0.01, 'kernel': 'rbf'}           0.547619   \n",
      "13      {'C': 1.0, 'gamma': 0.01, 'kernel': 'linear'}           0.500000   \n",
      "14          {'C': 1.0, 'gamma': 0.1, 'kernel': 'rbf'}           0.666667   \n",
      "15       {'C': 1.0, 'gamma': 0.1, 'kernel': 'linear'}           0.500000   \n",
      "16      {'C': 10.0, 'gamma': 0.0001, 'kernel': 'rbf'}           0.476190   \n",
      "17   {'C': 10.0, 'gamma': 0.0001, 'kernel': 'linear'}           0.500000   \n",
      "18       {'C': 10.0, 'gamma': 0.001, 'kernel': 'rbf'}           0.523810   \n",
      "19    {'C': 10.0, 'gamma': 0.001, 'kernel': 'linear'}           0.500000   \n",
      "20        {'C': 10.0, 'gamma': 0.01, 'kernel': 'rbf'}           0.571429   \n",
      "21     {'C': 10.0, 'gamma': 0.01, 'kernel': 'linear'}           0.500000   \n",
      "22         {'C': 10.0, 'gamma': 0.1, 'kernel': 'rbf'}           0.595238   \n",
      "23      {'C': 10.0, 'gamma': 0.1, 'kernel': 'linear'}           0.500000   \n",
      "24     {'C': 100.0, 'gamma': 0.0001, 'kernel': 'rbf'}           0.500000   \n",
      "25  {'C': 100.0, 'gamma': 0.0001, 'kernel': 'linear'}           0.500000   \n",
      "26      {'C': 100.0, 'gamma': 0.001, 'kernel': 'rbf'}           0.547619   \n",
      "27   {'C': 100.0, 'gamma': 0.001, 'kernel': 'linear'}           0.500000   \n",
      "28       {'C': 100.0, 'gamma': 0.01, 'kernel': 'rbf'}           0.547619   \n",
      "29    {'C': 100.0, 'gamma': 0.01, 'kernel': 'linear'}           0.500000   \n",
      "30        {'C': 100.0, 'gamma': 0.1, 'kernel': 'rbf'}           0.571429   \n",
      "31     {'C': 100.0, 'gamma': 0.1, 'kernel': 'linear'}           0.500000   \n",
      "32    {'C': 1000.0, 'gamma': 0.0001, 'kernel': 'rbf'}           0.500000   \n",
      "33  {'C': 1000.0, 'gamma': 0.0001, 'kernel': 'line...           0.500000   \n",
      "34     {'C': 1000.0, 'gamma': 0.001, 'kernel': 'rbf'}           0.571429   \n",
      "35  {'C': 1000.0, 'gamma': 0.001, 'kernel': 'linear'}           0.500000   \n",
      "36      {'C': 1000.0, 'gamma': 0.01, 'kernel': 'rbf'}           0.595238   \n",
      "37   {'C': 1000.0, 'gamma': 0.01, 'kernel': 'linear'}           0.500000   \n",
      "38       {'C': 1000.0, 'gamma': 0.1, 'kernel': 'rbf'}           0.571429   \n",
      "39    {'C': 1000.0, 'gamma': 0.1, 'kernel': 'linear'}           0.500000   \n",
      "40   {'C': 10000.0, 'gamma': 0.0001, 'kernel': 'rbf'}           0.547619   \n",
      "41  {'C': 10000.0, 'gamma': 0.0001, 'kernel': 'lin...           0.500000   \n",
      "42    {'C': 10000.0, 'gamma': 0.001, 'kernel': 'rbf'}           0.619048   \n",
      "43  {'C': 10000.0, 'gamma': 0.001, 'kernel': 'line...           0.500000   \n",
      "44     {'C': 10000.0, 'gamma': 0.01, 'kernel': 'rbf'}           0.547619   \n",
      "45  {'C': 10000.0, 'gamma': 0.01, 'kernel': 'linear'}           0.500000   \n",
      "46      {'C': 10000.0, 'gamma': 0.1, 'kernel': 'rbf'}           0.571429   \n",
      "47   {'C': 10000.0, 'gamma': 0.1, 'kernel': 'linear'}           0.500000   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  \\\n",
      "0            0.523810           0.547619           0.536585   \n",
      "1            0.904762           0.785714           0.756098   \n",
      "2            0.523810           0.547619           0.536585   \n",
      "3            0.904762           0.785714           0.756098   \n",
      "4            0.595238           0.690476           0.780488   \n",
      "5            0.904762           0.785714           0.756098   \n",
      "6            0.523810           0.595238           0.536585   \n",
      "7            0.904762           0.785714           0.756098   \n",
      "8            0.523810           0.547619           0.536585   \n",
      "9            0.880952           0.761905           0.731707   \n",
      "10           0.785714           0.785714           0.731707   \n",
      "11           0.880952           0.761905           0.731707   \n",
      "12           0.785714           0.666667           0.658537   \n",
      "13           0.880952           0.761905           0.731707   \n",
      "14           0.809524           0.642857           0.707317   \n",
      "15           0.880952           0.761905           0.731707   \n",
      "16           0.785714           0.785714           0.756098   \n",
      "17           0.880952           0.761905           0.756098   \n",
      "18           0.857143           0.761905           0.756098   \n",
      "19           0.880952           0.761905           0.756098   \n",
      "20           0.785714           0.666667           0.658537   \n",
      "21           0.880952           0.761905           0.756098   \n",
      "22           0.833333           0.666667           0.804878   \n",
      "23           0.880952           0.761905           0.756098   \n",
      "24           0.880952           0.785714           0.756098   \n",
      "25           0.880952           0.761905           0.756098   \n",
      "26           0.809524           0.666667           0.707317   \n",
      "27           0.880952           0.761905           0.756098   \n",
      "28           0.714286           0.619048           0.731707   \n",
      "29           0.880952           0.761905           0.756098   \n",
      "30           0.761905           0.642857           0.804878   \n",
      "31           0.880952           0.761905           0.756098   \n",
      "32           0.833333           0.761905           0.756098   \n",
      "33           0.880952           0.761905           0.756098   \n",
      "34           0.738095           0.690476           0.658537   \n",
      "35           0.880952           0.761905           0.756098   \n",
      "36           0.761905           0.642857           0.756098   \n",
      "37           0.880952           0.761905           0.756098   \n",
      "38           0.761905           0.642857           0.804878   \n",
      "39           0.880952           0.761905           0.756098   \n",
      "40           0.809524           0.642857           0.731707   \n",
      "41           0.880952           0.785714           0.731707   \n",
      "42           0.642857           0.666667           0.682927   \n",
      "43           0.880952           0.785714           0.731707   \n",
      "44           0.785714           0.619048           0.829268   \n",
      "45           0.880952           0.785714           0.731707   \n",
      "46           0.761905           0.642857           0.804878   \n",
      "47           0.880952           0.785714           0.731707   \n",
      "\n",
      "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
      "0            0.536585         0.533682        0.009011               46  \n",
      "1            0.439024         0.677120        0.177714                6  \n",
      "2            0.536585         0.533682        0.009011               46  \n",
      "3            0.439024         0.677120        0.177714                6  \n",
      "4            0.341463         0.619628        0.150921               44  \n",
      "5            0.439024         0.677120        0.177714                6  \n",
      "6            0.536585         0.552729        0.026511               45  \n",
      "7            0.439024         0.677120        0.177714                6  \n",
      "8            0.536585         0.533682        0.009011               46  \n",
      "9            0.414634         0.657840        0.173279               20  \n",
      "10           0.365854         0.638560        0.167046               41  \n",
      "11           0.414634         0.657840        0.173279               20  \n",
      "12           0.463415         0.624390        0.110259               43  \n",
      "13           0.414634         0.657840        0.173279               20  \n",
      "14           0.487805         0.662834        0.104447               15  \n",
      "15           0.414634         0.657840        0.173279               20  \n",
      "16           0.365854         0.633914        0.177622               42  \n",
      "17           0.390244         0.657840        0.182639               20  \n",
      "18           0.439024         0.667596        0.158477               12  \n",
      "19           0.390244         0.657840        0.182639               20  \n",
      "20           0.682927         0.673055        0.068365               11  \n",
      "21           0.390244         0.657840        0.182639               20  \n",
      "22           0.634146         0.706852        0.094832                1  \n",
      "23           0.390244         0.657840        0.182639               20  \n",
      "24           0.414634         0.667480        0.178540               13  \n",
      "25           0.390244         0.657840        0.182639               20  \n",
      "26           0.512195         0.648664        0.108142               39  \n",
      "27           0.390244         0.657840        0.182639               20  \n",
      "28           0.609756         0.644483        0.068871               40  \n",
      "29           0.390244         0.657840        0.182639               20  \n",
      "30           0.634146         0.683043        0.086633                2  \n",
      "31           0.390244         0.657840        0.182639               20  \n",
      "32           0.414634         0.653194        0.164461               38  \n",
      "33           0.390244         0.657840        0.182639               20  \n",
      "34           0.658537         0.663415        0.054440               14  \n",
      "35           0.390244         0.657840        0.182639               20  \n",
      "36           0.634146         0.678049        0.068040                5  \n",
      "37           0.390244         0.657840        0.182639               20  \n",
      "38           0.634146         0.683043        0.086633                2  \n",
      "39           0.390244         0.657840        0.182639               20  \n",
      "40           0.536585         0.653659        0.105312               37  \n",
      "41           0.414634         0.662602        0.176373               16  \n",
      "42           0.658537         0.654007        0.021742               36  \n",
      "43           0.414634         0.662602        0.176373               16  \n",
      "44           0.585366         0.673403        0.112636               10  \n",
      "45           0.414634         0.662602        0.176373               16  \n",
      "46           0.634146         0.683043        0.086633                2  \n",
      "47           0.414634         0.662602        0.176373               16  \n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "\n",
    "    X, y = principalDf1, labels\n",
    "\n",
    "    param_grid = {'C' : [0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0], 'gamma' : [1e-4, 1e-3, 1e-2, 1e-1], 'kernel': ('rbf', 'linear')}\n",
    "    classifier = SVC()\n",
    "    grid_search = GridSearchCV(estimator=classifier, param_grid=param_grid, scoring='accuracy', n_jobs=-1, refit = True, verbose=0)\n",
    "    grid_search.fit(X,y)\n",
    "    print(pd.DataFrame(grid_search.cv_results_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7884615384615384\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.83      0.78        24\n",
      "           0       0.84      0.75      0.79        28\n",
      "\n",
      "    accuracy                           0.79        52\n",
      "   macro avg       0.79      0.79      0.79        52\n",
      "weighted avg       0.79      0.79      0.79        52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(principalDf1, labels, train_size=0.75, random_state=8675309)\n",
    "model = SVC(C = 10.0, gamma = .1, kernel = 'rbf')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "cm_display.plot()\n",
    "plt.show()\n",
    "print(sklearn.metrics.classification_report(y_test, y_pred, labels = [1,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 5.3 (1 point):** What is the total explained variance ratio captured by this PCA? How accurate is this model? What evidence are you using to determine that? How many false positives and false negatives does it predict? How does it compare to the 3 PCA component model? To the full feature model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> The total eplained variance ratio for this model is 62.54%. It is significantly higher than with 3 components. The accuracy is 79% accurate using accuracy score and classification report. It predicts 7 false positives again, but only 4 falso negatives. It is better than the 3 PCA model by around 12% accuracy, but still around 9% less accurate than the full feature model in part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository `hw04_branch` using the commit message \"Committing Part 5\", and push the changes to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. How well does PCA work? (12 points)\n",
    "\n",
    "Clearly, the number of components we use in our PCA matters. Let's investigate how they matter by systematically building a model for any number of selected components. While this might seem a bit unnecessary for such a relatively small dataset, **this can be very useful for more complex datasets and models!**\n",
    "\n",
    "### 6.1 Accuracy vs. Components\n",
    "\n",
    "To systematically explore how well PCA improves our classification model, we will do this by writing a function that creates the PCA, the SVC model, fits the training data, predict the labels using test data, and returns the accuracy scores and the explained variance ratio. So your function will take as input:\n",
    "* the number of requested PCA components\n",
    "* the training feature data\n",
    "* the testing feature data\n",
    "* the training data labels\n",
    "* the test data labels\n",
    "\n",
    "and it should **return** the accuracy score for an SVC model fit to pca transformed features and the **total** explained variance ratio (i.e. the sum of the explained variance for each component).\n",
    "\n",
    "**&#9989; Task 6.1 (4 points):** Create this function, which you will use in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code here\n",
    "def func(n_components,X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    xfeatures = StandardScaler().fit_transform(features)\n",
    "    pca = PCA(n_components)\n",
    "    principalComponents = pca.fit_transform(xfeatures)\n",
    "    principalDf = pd.DataFrame(data = principalComponents\n",
    "             , num_columns = n_components)\n",
    "    finalDf = pd.concat([principalDf, labels], axis = 1)\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(principalDf, labels, train_size=0.75, random_state=8675309)\n",
    "    \n",
    "    if __name__=='__main__':\n",
    "\n",
    "        X, y = principalDf, labels\n",
    "\n",
    "        param_grid = {'C' : [0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0], 'gamma' : [1e-4, 1e-3, 1e-2, 1e-1], 'kernel': ('rbf', 'linear')}\n",
    "        classifier = SVC()\n",
    "        grid_search = GridSearchCV(estimator=classifier, param_grid=param_grid, scoring='accuracy', n_jobs=-1, refit = True, verbose=0)\n",
    "        grid_search.fit(X,y)\n",
    "        grid_search.cv_results_\n",
    "    \n",
    "    model = SVC(rank_test_score == 1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return(\"Accuracy:\", accuracy)\n",
    "    return('total explained variance ratio:', sum(evr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PCA() missing 4 required positional arguments: 'X_train', 'X_test', 'y_train', and 'y_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-228-d1f393f4d144>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprincipalDf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8675309\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-226-54b6c7f3bf6e>\u001b[0m in \u001b[0;36mfunc\u001b[0;34m(n_components, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mxfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprincipalComponents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     principalDf = pd.DataFrame(data = principalComponents\n",
      "\u001b[0;31mTypeError\u001b[0m: PCA() missing 4 required positional arguments: 'X_train', 'X_test', 'y_train', and 'y_test'"
     ]
    }
   ],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(principalDf, labels, train_size=0.75, random_state=8675309)\n",
    "func(1,X_train1, X_test1, y_train1, y_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Compute accuracies\n",
    "\n",
    "Now that you have created a function that returns the accuracy for a given number of components, we will use that to plot the how the accuracy of your SVC model changes when we increase the number of components used in the PCA.\n",
    "\n",
    "**&#9989; Task 6.2 (2 points):** Going from **1 to 15** components (so up to 25% of the original number of features), use your function above to compute and store (as a list) the accuracy of your models and the total explained variance ratio of your models.\n",
    "\n",
    "**Note**: you'll be running many grid searches to do this, so it might take your computer a bit of time to run all of these models. Please be patient. It shouldn't more than a couple minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PCA() missing 4 required positional arguments: 'X_train', 'X_test', 'y_train', and 'y_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-229-bbea99b0ff31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-226-54b6c7f3bf6e>\u001b[0m in \u001b[0;36mfunc\u001b[0;34m(n_components, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mxfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprincipalComponents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     principalDf = pd.DataFrame(data = principalComponents\n",
      "\u001b[0;31mTypeError\u001b[0m: PCA() missing 4 required positional arguments: 'X_train', 'X_test', 'y_train', and 'y_test'"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "x = []\n",
    "i = 1\n",
    "while i < 15:\n",
    "    x.append(func(i,X_train1, X_test1, y_train1, y_test1))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Plot accuracy vs number of components\n",
    "\n",
    "Now that we have those numbers, it makes sense to look at the accuracy vs # of components.\n",
    "\n",
    "**&#9989; Task 6.3 (2 points):** Plot the accuracy vs # of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code here\n",
    "plt.plot(accuracy,n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 6.1 (1 point):** What do you observe about the accuracy as a function of the number of PCA components you use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> Wish i got it to work but I imagine it would be similar to the square root graph. The accuracy increases as the number of components do, but less of an increase for each new component added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Plot total explained variance vs number of components\n",
    "\n",
    "What if we look at total explained variance as a function of # of components?\n",
    "\n",
    "**&#9989; Task 6.4 (2 points):** Plot the total explained variance ratio vs # of components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_components' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-230-d38baeb0c373>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Put your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'n_components' is not defined"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "plt.plot(evr,n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 6.2 (1 points):** At what point does it seem like we start to have diminishing returns, that is, no major increase in explained variance as we add additional components to the PCA? How does the shape of this curve compare to the one of accuracy from above? Why do you think this is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> I would guess around 67% of components would have no major increase because almost all explained variance would be there. It is a sharper decline but similar becuase almost no new variance is explained, but the accuracy still increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository `hw04_branch` using the commit message \"Committing Part 6\", and push the changes to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Revisiting the Perceptron classifier with code created by generative AI (12 points)\n",
    "\n",
    "After working through the Perceptron classifier in class, one of your classmates decided to try and see if they could get ChatGPT to write a Perceptron classifier that they could test out using the sonar dataset. ChatGPT produced the code provided in the Python script that you can download from here:\n",
    "\n",
    "`https://raw.githubusercontent.com/msu-cmse-courses/cmse202-supplemental-data/main/code_samples/perceptron.py`\n",
    "\n",
    "After reviewing the code, you and your classmate agree that the classifier should work, but your classmate is running into some struggles with figuring out how to pass the data to the code from ChatGPT to train and test the classifier. They've asked you to help them out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Do this**: Download the Python script from the URL above and add it to your repository. You should add it to the same directory as this notebook. You should also add and commit it to your repository, making sure it is in your `hw04_branch` branch.\n",
    "\n",
    "Once you've downloaded the file, put it in the right place, and committed it to your repository, you should be able to import it into this notebook and use it to train and test a Perceptron classifier.\n",
    "\n",
    "**Run the following cell to import the code from the Python script.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perceptron import Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Task 7.1 (5 points):** Create an instance of the `Perceptron` object defined by the class in the Python script using a **learning rate of 0.1** and **10 iterations**. Then, use the `fit()` to train the classifier using the training features and labels from the sonar dataset you've been using in the assignment up to this point. Finally, use the `predict()` method to predict the labels for the test features and print the accuracy score.\n",
    "\n",
    "**Hint**: the script produced by ChatGPT includes some example code that you can use to figure out how to use the `Perceptron` class. It also includes a function for printing the accuracy score that you can use, if you want, or you can use the same one you've been using previously.\n",
    "\n",
    "**Note**: You may run into an error when you try to run the fit method, you'll have to debug this error to get the provided class to work. **You should not need to change the code itself**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron classification accuracy 0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "if __name__ == \"__main__\":\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import datasets\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    def accuracy(y_true, y_pred):\n",
    "        accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "        return accuracy\n",
    "\n",
    "    X, y = npfeatures, nplabels\n",
    "\n",
    "    # Binary classification, so let's take only two classes\n",
    "    X = X[y != 2]\n",
    "    y = y[y != 2]\n",
    "\n",
    "    X_Train, X_Test, y_Train, y_Test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "    p = Perceptron(learning_rate=.1, n_iters=10)\n",
    "    p.fit(X_Train, y_Train)\n",
    "    predictions = p.predict(X_Test)\n",
    "\n",
    "    print(\"Perceptron classification accuracy\", accuracy(y_Test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "npfeatures = np.asarray(features)\n",
    "nplabels = np.asarray(labels)\n",
    "nplabels = nplabels.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 7.1 (1 points):** How well of job did the Perceptron classifier do on the sonar dataset? How does it compare to the SVC model you built in the previous parts of this assignment? Why do you think this is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was 71.4% accurate, which is a little worse than the SVC model. The SVC model had 75% of the data as training, while the Perceptron had 80%. This would lead me to believe that perceptron would be better. However; it is not, so maybe the random state of 8675309 is superior to 123 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Task 7.2 (5 points):** Come up with a way to do your own \"grid search\" for the Perceptron classifier written by ChatGPT. **Test a range of learning rate values from 0.001 to 1.0. Experiment with 10, 100, and 1000 iterations.**  Try to find the best learning rate and number of iterations for the Perceptron.  What is the best accuracy you can get? Make sure you keep track of the learning rate and number of iterations that correspond to the best accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron classification accuracy 0.6190476190476191\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "if __name__ == \"__main__\":\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import datasets\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    def accuracy(y_true, y_pred):\n",
    "        accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "        return accuracy\n",
    "\n",
    "    x, Y = npfeatures, nplabels\n",
    "\n",
    "    # Binary classification, so let's take only two classes\n",
    "    x = x[Y != 2]\n",
    "    Y = Y[Y != 2]\n",
    "\n",
    "    x_Train, x_Test, Y_Train, Y_Test = train_test_split(x, Y, test_size=0.2, random_state=123)\n",
    "\n",
    "    p = Perceptron(learning_rate=.01, n_iters=1000)\n",
    "    p.fit(x_Train, Y_Train)\n",
    "    predictions = p.predict(x_Test)\n",
    "\n",
    "    print(\"Perceptron classification accuracy\", accuracy(Y_Test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**&#9989; Question 7.2 (1 point):** If you explore the various accuracies you end up getting for the various combinations of learning rate and number of iterations, you'd probably notice that there are a number of combinations that give you the same accuracy. This suggests that perhaps the Perceptron classifier is not finding a \"converged\" solution. As a reminder, the Perceptron classifier will only converge if the data is linearly separable. Do you think the sonar dataset is linearly separable? Why or why not? (You can use some of your experience with the SVC model to help you answer this question.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> The only thing that changes the accuracy is if the number of iterations is increased to 1000. This decreases the accuracy down to 61.9%. The learning rate has no affect on the accuracy, and the jump from 10 to 100 iterations does nothing as well. This leads me to believe that sonar dataset is not linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository `hw04_branch` using the commit message \"Committing Part 7\", and push the changes to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1. Continued\n",
    "\n",
    "Now that you've finished your new \"development\" on your 202 turn-in repo, you can merge your work back into your `main` branch.\n",
    "\n",
    "**&#9989; Do the following**:\n",
    "\n",
    "7. Switch back to your `main` branch. \n",
    "8. Merge your `hw04_branch` with your `main` branch. \n",
    "9. Finally, push the changes to GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment wrap-up¶\n",
    "Please fill out the form that appears when you run the code below. **You must completely fill this out in order to receive credit for the assignment!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\n",
    "\"\"\"\n",
    "<iframe \n",
    "\tsrc=\"https://forms.office.com/r/mHUMR9xxSH\" \n",
    "\twidth=\"800px\" \n",
    "\theight=\"600px\" \n",
    "\tframeborder=\"0\" \n",
    "\tmarginheight=\"0\" \n",
    "\tmarginwidth=\"0\">\n",
    "\tLoading...\n",
    "</iframe>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations, you're done!\n",
    "Submit this assignment by uploading it to the course Desire2Learn web page. Go to the \"Homework Assignments\" folder, find the submission folder for Homework 4, and upload your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#169; Copyright 2023,  Department of Computational Mathematics, Science and Engineering at Michigan State University"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
